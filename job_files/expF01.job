#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus-per-node=1
#SBATCH --job-name=rf_expf01
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=10:00:00
#SBATCH --mem=120000M
#SBATCH --output=slurm_output_%A.out

module purge
module load 2022
module load Anaconda3/2022.05

source activate rib

DATA_ROOT=/scratch-shared/$USER/data/

WANDB_KEY=

cd ~/rib-fracture

python train.py --exp-name expf01 --use-model unet3plus-ds --context-size 8 --do-wandb --wandb-key $WANDB_KEY --data-root $DATA_ROOT --download-data  --max-epochs 3 --batch-size-train 32 --batch-size-test 32 --patch-final-size 128
