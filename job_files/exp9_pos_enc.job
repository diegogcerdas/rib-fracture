#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus-per-node=1
#SBATCH --job-name=rf_exp0
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=10:00:00
#SBATCH --mem=120000M
#SBATCH --output=slurm_output_%A.out

module purge
module load 2022
module load Anaconda3/2022.05

source activate rib

DATA_ROOT=/scratch-shared/$USER/data/

WANDB_KEY=
WANDB_USER=

cd ~/rib-fracture

python train.py --exp-name exp0 --use-model unet3plus --context-size 0 --positional-encoding True --do-wandb --wandb-key $WANDB_KEY --wandb-entity $WANDB_USER --data-root $DATA_ROOT --download-data  --max-epochs 10 --batch-size-train 32 --batch-size-test 32 --patch-final-size 128
