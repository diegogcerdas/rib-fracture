#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus-per-node=1
#SBATCH --job-name=final02
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=60:00:00
#SBATCH --mem=120000M
#SBATCH --output=slurm_output_%A.out

module purge
module load 2022
module load Anaconda3/2022.05

source activate rib

DATA_ROOT=/scratch-shared/$USER/data/

WANDB_KEY=

cd ~/rib-fracture

python train.py --exp-name expf02_final --wandb-id ti8bvv1n --resume-ckpt checkpoints/expf02_final/last.ckpt --use-model unet3plus-ds --context-size 8 --use-msssim-loss --do-wandb --wandb-key $WANDB_KEY --data-root $DATA_ROOT --download-data  --max-epochs 50 --batch-size-train 32 --batch-size-test 32 --patch-final-size 128
